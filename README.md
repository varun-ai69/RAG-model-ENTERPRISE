# WOFO ~ RAG-Based Enterprise Knowledge Offline Assistant 





A **Retrieval-Augmented Generation (RAG)** system designed for organizations to securely upload internal documents (policies, manuals, legal docs, FAQs, etc.) and allow employees to query them using natural language.

This system converts private documents into searchable vector embeddings and uses a Large Language Model (LLM) to generate accurate, context-aware answers â€” **strictly based on company data**.

---

## Key Features

*  Upload PDFs, DOCX, TXT files
*  Automatic text extraction & chunking
*  Semantic search using vector embeddings
*  LLM-powered question answering
*  Company-wise isolated data access
*  Role-based access (Admin/Employee)
*  Chat and memory stores in database
*  Fast similarity search using Qdrant
*  Modular & scalable architecture

---

## System Architecture (High-Level)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Admin Uploads Docs   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                  Text Extraction & Chunking
                           â”‚
                  Embedding Generation (HF)
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Vector Database     â”‚  â† Qdrant
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
       User Query â†’ Embed â†’ Similarity Search
                           â”‚
                    Relevant Context
                           â”‚
                   LLM (Gemini / LLaMA)
                           â”‚
                     Final Answer
```

---

##  Tech Stack

### Backend

* **Node.js**
* **Express.js**

### Frontend

* **React**

### AI / ML

* **Embedding Model:** HuggingFace Sentence Transformer
* **LLM:** Gemini 3 Flash (can be replaced with LLaMA)
* **Vector DB:** Qdrant

### Database

* **MongoDB** â€“ Users, Chats, Metadata

### Tools

* **Docker** â€“ Running Qdrant
* **Postman** â€“ API Testing

---

## ğŸ”„ RAG FLOW EXPLAINED

### 1ï¸âƒ£ Ingestion Pipeline

Used when **admin uploads documents**.

1. Extract text from PDF/DOCX 
2. Clean & normalize text 
3. Split into semantic chunks 
4. Convert chunks into embeddings for embedding we are using HuggingFace Sentence Transformer model 
5. Store embeddings in **Qdrant** (Qdrant is our VectorDB that stores the embededChunks and its metadata)

---

### 2ï¸âƒ£ Query Pipeline (User Chat)

1. User enters a question
2. Question is converted to vector using same embedding model 
3. Vector similarity search in Qdrant
4. Top-N relevant chunks retrieved
5. It filter the chunks and create context for LLM 
6. Context + question passed to LLM ( currently we use gemini-3-flash-preview )
7. LLM generates accurate answer

---

## ğŸ§  Example Flow

```
User: "What is the company leave policy?"

â†’ Embed question 
â†’ Search vector DB
â†’ Retrieve related policy chunks
â†’ Send to LLM
â†’ Return summarized answer
```

---

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone <repo-url>
cd project
```

### 2ï¸âƒ£ Install Dependencies

```bash
npm install
```

### 3ï¸âƒ£ Setup Environment Variables

Create `.env` file:

```env
PORT=3000
MONGO_URI=your_mongodb_url
JWT_SECRET=your_secret
GEMINI_API_KEY=your_gemini_key
```

---

### 4ï¸âƒ£ Run Vector Database (Qdrant)

```bash
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

---

### 5ï¸âƒ£ Start Backend Server

```bash
node server.js
```

---

## ğŸ§ª API Endpoints

## 1. Core RAG APIs (Primary APIs)

These are the **main APIs** responsible for document ingestion and intelligent querying.

### 1.1 Upload Document (Ingestion)

**Endpoint:**

`POST /api/upload/ingestion`

**Description:**

Uploads documents (PDF, DOCX, etc.) and processes them through the ingestion pipeline.

The system extracts text, creates embeddings, and stores them in the vector database.

---

### 1.2 Ask Question (Retrieval)

**Endpoint:**

`POST /api/search/retrieval`

**Request Body:**

```json
{
  "question": "What is the leave policy?"
}

```

**Description:**

Processes the user query by generating embeddings, retrieving relevant document chunks, and generating an answer using the LLM.

---

## 2. Authentication APIs

### 2.1 User Login

`POST /api/auth/login`

Authenticates a user and returns a JWT token.

---

### 2.2 User Registration

`POST /api/auth/register`

Registers a new user in the system.

---

## Technologies Used

| Component      | Technology       |
| -------------- | ---------------- |
| Backend        | Node.js, Express |
| Vector DB      | Qdrant           |
| Embeddings     | HuggingFace      |
| LLM            | Gemini 3 Flash   |
| Database       | MongoDB          |
| Authentication | JWT              |
| Deployment     | Docker           |

---


## Final Notes

This system is designed to be:

* Secure
*  Fast
*  Intelligent
*  Scalable

It enables organizations to **safely query their private data using AI** â€” without leaking information externally.

---
